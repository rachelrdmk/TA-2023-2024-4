{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30fd10e-d80b-411c-929a-75c2346218ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Reshape, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, Conv2D, Conv2DTranspose, UpSampling2D\n",
    "# from keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "# from keras.engine.saving import load_model\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "375d4549-f76a-4201-9df9-f0053efb9dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_image_files(directory):\n",
    "    files = sorted(os.listdir(directory))\n",
    "    return [os.path.join(directory, f) for f in files if is_an_image_file(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edb503b0-62aa-43d1-a3c0-4e6e788ecd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_an_image_file(filename):\n",
    "    IMAGE_EXTENSIONS = ['.png', '.jpg', '.jpeg']\n",
    "    for ext in IMAGE_EXTENSIONS:\n",
    "        if ext in filename:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f1585e3-cedf-4ae1-93dc-8b5b8014f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = cv2.imread(path[0])\n",
    "    \n",
    "    # Make sure all images are 256 x 256 by cropping them\n",
    "    r, c = img.shape[:2]\n",
    "    r_diff = (r - 256) // 2\n",
    "    c_diff = (c - 256) // 2\n",
    "    cropped = img[r_diff:256 + r_diff, c_diff:256 + c_diff] \n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dc72d23-7148-4ef8-924c-5fa7b0ba34b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path, n_images=-1, shouldShuffle=False):\n",
    "    all_image_paths = list_image_files(path)\n",
    "    if shouldShuffle:\n",
    "        random.shuffle(all_image_paths)\n",
    "    \n",
    "    if n_images < 0:\n",
    "        n_images = len(all_image_paths)\n",
    "    images_l, images_ab = [], []\n",
    "    \n",
    "    # Initialize a progress bar with max of n_images\n",
    "    pbar = tqdm_notebook(total = n_images, desc=\"Loading Images...\")\n",
    "    \n",
    "    for path in zip(all_image_paths):\n",
    "        img = load_image(path)\n",
    "        lab_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "        lab_img = preprocess_image(lab_img)\n",
    "        \n",
    "        l = lab_img[:,:,0]\n",
    "        l = l[:,:,np.newaxis]\n",
    "        # Include all 3 channels, overwrite 1st channel with 0's\n",
    "        ab = lab_img[:,:,1:]\n",
    "\n",
    "        images_l.append(l)\n",
    "        images_ab.append(ab)\n",
    "\n",
    "        images_loaded = len(images_l)\n",
    "        \n",
    "        # Increase progress by one\n",
    "        pbar.update(1)\n",
    "        \n",
    "        if images_loaded > n_images - 1: \n",
    "            break\n",
    "\n",
    "    return {\n",
    "        'l': np.array(images_l),\n",
    "        'ab': np.array(images_ab)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4b95937-4836-4c1e-af2f-84b15e1a7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESHAPE = (256,256)\n",
    "\n",
    "def preprocess_image(cv_img):\n",
    "    img = (cv_img - 127.5) / 127.5\n",
    "    return img\n",
    "\n",
    "def deprocess_image(img):\n",
    "    img = (img * 127.5) + 127.5\n",
    "    return img.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "effedf3b-ebdc-4652-9c4b-584428975d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(np_arr, path):\n",
    "    img = np_arr * 127.5 + 127.5\n",
    "    im = Image.fromarray(img)\n",
    "    im.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7239c33b-4ba3-41be-8735-65c570b7ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(H, W, k):\n",
    "    # Inputs: height and width of the input image\n",
    "    # Returns the model, which generates the AB channels\n",
    "\n",
    "    # Pix2pix adapted from \n",
    "    # https://github.com/eriklindernoren/Keras-GAN/blob/master/pix2pix/pix2pix.py\n",
    "\n",
    "    def conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "        \"\"\"Layers used during downsampling\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if bn:\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "        return d\n",
    "\n",
    "    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "        \"\"\"Layers used during upsampling\"\"\"\n",
    "        u = UpSampling2D(size=2)(layer_input)\n",
    "        u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "        if dropout_rate:\n",
    "            u = Dropout(dropout_rate)(u)\n",
    "        u = BatchNormalization(momentum=0.8)(u)\n",
    "        u = Concatenate()([u, skip_input])\n",
    "        return u\n",
    "\n",
    "    gf = 64 # Number of filters in the first layer of G\n",
    "\n",
    "    noise_in = Input(shape=(100,))\n",
    "    condition_in = Input(shape=(H, W, 1))\n",
    "    \n",
    "    # pass noise through a FC layer to get it to the right size\n",
    "    noise = Dense(H * H)(noise_in)\n",
    "\n",
    "    # reshape to be the size of an image channel\n",
    "    noise = Reshape((H, H, 1))(noise)\n",
    "    \n",
    "    # stick the (somewhat modified) noise as the second channel after\n",
    "    # the gray input. Assuming new dimension of hid will be\n",
    "    # B x 256 x 256 x 2, where B is the batch size.\n",
    "    d0 = Concatenate(axis=-1)([condition_in, noise])\n",
    "#     d0 = condition_in # Don't need noise since it's being ignored anyway\n",
    "\n",
    "    # U-NET\n",
    "    # Downsampling\n",
    "    d1 = conv2d(d0, gf, bn=False)\n",
    "    d2 = conv2d(d1, gf*2)\n",
    "    d3 = conv2d(d2, gf*4)\n",
    "    d4 = conv2d(d3, gf*8)\n",
    "    d5 = conv2d(d4, gf*8)\n",
    "    d6 = conv2d(d5, gf*8)\n",
    "    d7 = conv2d(d6, gf*8)\n",
    "\n",
    "    # Upsampling\n",
    "    u1 = deconv2d(d7, d6, gf*8)\n",
    "    u2 = deconv2d(u1, d5, gf*8)\n",
    "    u3 = deconv2d(u2, d4, gf*8)\n",
    "    u4 = deconv2d(u3, d3, gf*4)\n",
    "    u5 = deconv2d(u4, d2, gf*2)\n",
    "    u6 = deconv2d(u5, d1, gf)\n",
    "\n",
    "    u7 = UpSampling2D(size=2)(u6)\n",
    "    \n",
    "    # Final 2-channel AB image with values between -1 and 1\n",
    "    img_out = Conv2D(2*k, kernel_size=4, strides=1, padding='same', activation='tanh', name='pred_ab')(u7)\n",
    "\n",
    "    # Make Model\n",
    "    model = Model(inputs=[noise_in, condition_in], outputs=img_out)\n",
    "    \n",
    "    # Show summary of layers\n",
    "    print(\"Generator Model:\")\n",
    "    model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53672043-f13c-4ddf-ae27-1a8ef3a73edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator(H, W, k):\n",
    "    # Inputs: height and width of the input image\n",
    "    # Returns the model, which predicts real/fake\n",
    "    # over a set of spatial regions (i.e., predicts a matrix instead of a scalar).\n",
    "\n",
    "    # Pix2pix adapted from \n",
    "    # https://github.com/eriklindernoren/Keras-GAN/blob/master/pix2pix/pix2pix.py\n",
    "\n",
    "    def d_layer(layer_input, filters, f_size=4, bn=True):\n",
    "        \"\"\"Discriminator layer\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if bn:\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "        return d\n",
    "\n",
    "    # Number of filters in the first layer of D\n",
    "    df = 64\n",
    "\n",
    "    img_in = Input(shape=(H, W, 2*k)) # AB channels\n",
    "    condition_in = Input(shape=(H, W, 1)) # L channel\n",
    "    \n",
    "    # Concat the L and AB channels\n",
    "    concat_imgs = Concatenate()([condition_in, img_in])\n",
    "\n",
    "    d1 = d_layer(concat_imgs, df, bn=False)\n",
    "    d2 = d_layer(d1, df*2)\n",
    "    d3 = d_layer(d2, df*4)\n",
    "    d4 = d_layer(d3, df*8)\n",
    "\n",
    "    # validity map is a one-channel matrix 1/16 the size of the input (halved 4 times).\n",
    "    # Each number predicts whether a region of the input is real/fake.\n",
    "    validity = Conv2D(1*k, kernel_size=4, strides=1, padding='same', name='pred_valid')(d4)\n",
    "\n",
    "    # Build Model\n",
    "    model = Model(inputs=[img_in, condition_in], outputs=validity)\n",
    "\n",
    "    # Show summary of layers\n",
    "    print(\"Disciminator Model:\")\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "462b64cd-121f-44b6-9433-e6cbac452304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_k_diff(y_true, y_pred):\n",
    "    # Shape: (Batch, H, W, k, 2)\n",
    "    y_true = K.reshape(y_true, (-1, H, W, k, 2))\n",
    "    y_pred = K.reshape(y_pred, (-1, H, W, k, 2))\n",
    "\n",
    "    print(\"true:\", y_true.shape)\n",
    "    print(\"pred:\", y_pred.shape)\n",
    "\n",
    "    diff = y_true - y_pred\n",
    "    diff = K.abs(diff)\n",
    "    diff = K.mean(diff, axis=(1, 2, 4)) # mean of (H, W, 2) leaves (B, k)\n",
    "    \n",
    "    loss_metric = diff\n",
    "\n",
    "    min_for_each_batch = K.min(loss_metric, axis=1)\n",
    "    return K.sum(min_for_each_batch) #* .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e493d4e9-8152-48b1-80cd-28f2bf11f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "def generate_noise(n_samples, noise_dim):\n",
    "    X = np.random.normal(0, 1, size=(n_samples, noise_dim))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44609c37-b973-449b-a289-9e3f234a8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_rgb_img(l, ab, filename):\n",
    "    # Make sure ab is the right type, generated imgs change to float32\n",
    "    ab = ab.astype(np.float64)\n",
    "    \n",
    "    # Merge\n",
    "    merged = cv2.merge((l, ab))\n",
    "    \n",
    "    # Get between 0, 255\n",
    "    deprocessed = deprocess_image(merged)\n",
    "    \n",
    "    # Change to BGR (Curse you CV2!!!)\n",
    "    rgb = cv2.cvtColor(deprocessed, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "    # Save\n",
    "    cv2.imwrite(save_path + filename, rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce345164-c71f-4ed6-8ecb-1bad446bb6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discrim_models():\n",
    "    # Load training data\n",
    "    max_imgs = 2500\n",
    "    if len(list_image_files(train_dataset)) > max_imgs:\n",
    "        num_imgs = max_imgs\n",
    "    else:\n",
    "        num_imgs = -1\n",
    "\n",
    "    train_data = load_images(train_dataset, num_imgs, True)\n",
    "    train_l, train_ab = train_data['l'], train_data['ab']\n",
    "\n",
    "    # Make generated data\n",
    "    noise = generate_noise(len(train_l), 100)\n",
    "    train_predictions = generator.predict([noise, train_l])\n",
    "\n",
    "    # Tile truth data\n",
    "    tiled = np.tile(train_ab, k)\n",
    "\n",
    "    # Make discrim predictions\n",
    "    generated_discrim_values = discriminator.predict([train_predictions, train_l])\n",
    "    true_discrim_values = discriminator.predict([tiled, train_l])\n",
    "    \n",
    "    # Flatten discrim values\n",
    "    n = generated_discrim_values.shape\n",
    "    flat_gen_discrim_val = generated_discrim_values.reshape((n[0], n[1] * n[2], n[3]))\n",
    "    flat_true_discrim_val = true_discrim_values.reshape((n[0], n[1] * n[2], n[3]))\n",
    "    \n",
    "    # Make labels\n",
    "    model_labels = np.concatenate((np.zeros(len(generated_discrim_values)), np.ones(len(true_discrim_values))))\n",
    "    \n",
    "    # Loop through and store models\n",
    "    models = []\n",
    "    for i in range(k):\n",
    "        model_data = np.concatenate((flat_gen_discrim_val[:,:,i], flat_true_discrim_val[:,:,i]))\n",
    "        model = LogisticRegression(max_iter=1000).fit(model_data, model_labels)\n",
    "        models.append(model)\n",
    "        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e2ce8a0-5e7b-45cc-b0c3-72fcf621f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "\n",
    "# Program will grab 100 epoch weights for G,D in ./Output/trained_model_name/\n",
    "# trained_model_name = \"lsun_colorization_full_model\"\n",
    "# trained_model_name = \"places2_colorization_read_imgs_flow_test\"\n",
    "trained_model_name = \"Output_final_k_1_with_noise\"\n",
    "# trained_model_name = 'ablation_circles_equal_l_k_3_no_noise_no_aug'\n",
    "\n",
    "# Program will save output in ./TestOutput/trained_model_name by default\n",
    "# Change from none to save output in ./TestOutput/overwrite_save_dir\n",
    "# overwrite_save_dir = \"WHJ_Predictions\"\n",
    "# overwrite_save_dir = \"LSUN_on_places2\"\n",
    "overwrite_save_dir = None\n",
    "\n",
    "# Should output have the same file names as the test images?\n",
    "preserve_img_names = True\n",
    "\n",
    "# Must match k from model\n",
    "k = 1\n",
    "\n",
    "# Testing parameters\n",
    "num_test_imgs = 100\n",
    "\n",
    "# Should program randomly colorize some test images and leave some ground truth?\n",
    "# This was used to generate data for a user study\n",
    "random_select_gt_or_colorzed = False\n",
    "\n",
    "# Specify what dataset to test on\n",
    "# dataset = 'circle_pairs_equal_l_red_blue/'\n",
    "# dataset = 'new_circles/'\n",
    "# dataset = '../Colorization_GAN/circle_pairs/'\n",
    "# test_dataset = 'lsun/test/'\n",
    "test_dataset = 'Test_Output/'\n",
    "# train_dataset = 'places2/train/subdir/'\n",
    "# test_dataset = 'places2/10_imgs_per_cat/'\n",
    "# dataset = 'William_Henry_Jackson/WHJ_Resized_Square/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2eac424b-cc55-44ec-897e-d29ba041cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find where model is located\n",
    "saved_GAN_location = \"Output/\" + trained_model_name + \"/GAN_Weights_Epoch_100.h5\"\n",
    "saved_D_location = \"Output/\" + trained_model_name + \"/Discriminator_Weights_Epoch_100.h5\"\n",
    "\n",
    "# Create folder to store output\n",
    "generic_output_folder = \"Test_Output/\"\n",
    "\n",
    "if overwrite_save_dir is None:\n",
    "    new_output_folder = trained_model_name + \"/\"\n",
    "    save_path = generic_output_folder + new_output_folder\n",
    "else:\n",
    "    save_path = generic_output_folder + overwrite_save_dir + \"/\"\n",
    "    \n",
    "if random_select_gt_or_colorzed:\n",
    "    save_path += \"random_colorized_or_ground_truth/\"\n",
    "else:\n",
    "    save_path += \"all_predictions/\"\n",
    "\n",
    "# Ensure output can save in desired location\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0d78816-4c7d-41ac-8902-478c675ab12f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'TA-2023-2024-4/Test_Output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ===================================\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# COULD NOT HANDLE LARGE TRAINING SET\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ===================================\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# data = load_images(dataset + 'test', num_test_imgs)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# data = load_images(dataset + \"/test/\", num_test_imgs)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_test_imgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Only want l channel\u001b[39;00m\n\u001b[0;32m     14\u001b[0m l_channel_imgs, ab_channel_imgs \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mab\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m, in \u001b[0;36mload_images\u001b[1;34m(path, n_images, shouldShuffle)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_images\u001b[39m(path, n_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, shouldShuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m----> 2\u001b[0m     all_image_paths \u001b[38;5;241m=\u001b[39m \u001b[43mlist_image_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shouldShuffle:\n\u001b[0;32m      4\u001b[0m         random\u001b[38;5;241m.\u001b[39mshuffle(all_image_paths)\n",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m, in \u001b[0;36mlist_image_files\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlist_image_files\u001b[39m(directory):\n\u001b[1;32m----> 2\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files \u001b[38;5;28;01mif\u001b[39;00m is_an_image_file(f)]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'TA-2023-2024-4/Test_Output'"
     ]
    }
   ],
   "source": [
    "# ===================================\n",
    "# COULD NOT HANDLE LARGE TRAINING SET\n",
    "# ===================================\n",
    "\n",
    "# Get training images\n",
    "# Load dataset, convert to LAB, normalize to range [-1, 1]\n",
    "# data = load_images(dataset + 'test', num_test_imgs)\n",
    "# data = load_images(dataset + \"/test/\", num_test_imgs)\n",
    "data = load_images(test_dataset, num_test_imgs)\n",
    "\n",
    "\n",
    "\n",
    "# Only want l channel\n",
    "l_channel_imgs, ab_channel_imgs = data['l'], data['ab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836248d3-9171-44fd-9e39-cccf0ac3f529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
